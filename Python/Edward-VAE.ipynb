{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This must be run with a kernel having tensorflow. Virtual-env tf works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE using Edward, a probabilistic programming language in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we do setup -- imports, data handling and model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from observations import mnist\n",
    "from matplotlib.image import imsave\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper for generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(array, batch_size):\n",
    "    \"\"\"Generate batch with respect to array's first axis.\"\"\"\n",
    "    start = 0  # pointer to where we are in iteration\n",
    "    while True:\n",
    "        stop = start + batch_size\n",
    "        diff = stop - array.shape[0]\n",
    "        if diff <= 0:\n",
    "            batch = array[start:stop]\n",
    "            start += batch_size\n",
    "        else:\n",
    "            batch = np.concatenate((array[start:], array[:diff]))\n",
    "            start = diff\n",
    "        batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n",
    "        batch = np.random.binomial(1, batch)  # Make images binary -- with small noise\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model parameters and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "ed.set_seed(123)\n",
    "\n",
    "# Define model sizes etc.\n",
    "z_dim = 2\n",
    "batch_size = 1000\n",
    "\n",
    "# DATA. MNIST batches are fed at training time.\n",
    "(x_train, _), (x_test, _) = mnist('./data')\n",
    "x_train_generator = generator(x_train, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, define the models -- both generative and variational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generative part  -- Taking us from the latent Z (Gaussian) to the observed X (Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a the generative  model\n",
    "# Sizes corresponding to a minibatch\n",
    "# Note the simple definition of variables\n",
    "z = ed.models.Normal(loc=tf.zeros([batch_size, z_dim]),\n",
    "                     scale=tf.ones([batch_size, z_dim]))\n",
    "hidden_gen = tf.layers.dense(z, 64, activation=tf.nn.relu)\n",
    "hidden_gen = tf.layers.dense(hidden_gen, 256, activation=tf.nn.relu)\n",
    "x = ed.models.Bernoulli(logits=tf.layers.dense(hidden_gen, 28 * 28, activation=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The variational model -- Taking us from a placeholder for the data to the latent Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variational approximation. \n",
    "# Again, sizes corresponding to a minibatch\n",
    "# x_ph is the Tensorflow placeholder that will be fed by data\n",
    "x_ph = tf.placeholder(tf.int32, [batch_size, 28 * 28])\n",
    "hidden_vb = tf.layers.dense(tf.cast(x_ph, tf.float32), 256,\n",
    "                            activation=tf.nn.relu)\n",
    "hidden_vb = tf.layers.dense(hidden_vb, 256,\n",
    "                            activation=tf.nn.relu)\n",
    "qz = ed.models.Normal(loc=tf.layers.dense(hidden_vb, z_dim, activation=None),\n",
    "                      scale=tf.layers.dense(\n",
    "                          hidden_vb, z_dim, activation=tf.nn.softplus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up inference machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind p(x, z) and q(z | x) to the same TensorFlow placeholder for x.\n",
    "# Edward has several inference engines, here we choose KLqp, which is the Variational Inference\n",
    "inference = ed.KLqp({z: qz}, data={x: x_ph})\n",
    "# Next, the solver is one of the standard solvers from Tensorflow\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n",
    "# Initialize the inference engine and the variables. \n",
    "inference.initialize(optimizer=optimizer)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the training -- simply iterate over epochs, then over mini-batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1: log p(x) >= -296.420\n",
      "Epoch:   2: log p(x) >= -196.557\n",
      "Epoch:   3: log p(x) >= -184.731\n",
      "Epoch:   4: log p(x) >= -175.976\n",
      "Epoch:   5: log p(x) >= -171.846\n",
      "Epoch:   6: log p(x) >= -168.593\n",
      "Epoch:   7: log p(x) >= -166.887\n",
      "Epoch:   8: log p(x) >= -164.657\n",
      "Epoch:   9: log p(x) >= -162.874\n",
      "Epoch:  10: log p(x) >= -161.167\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "n_iter_per_epoch = x_train.shape[0] // batch_size\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print(\"Epoch: {:3d}: \".format(epoch), end='')\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for t in range(1, n_iter_per_epoch + 1):\n",
    "        x_batch = next(x_train_generator)\n",
    "        info_dict = inference.update(feed_dict={x_ph: x_batch})\n",
    "        avg_loss += info_dict['loss']\n",
    "\n",
    "    # Print a lower bound to the average marginal likelihood for an\n",
    "    # image. The loss is -ELBO, so the log likelihood is lower-bunded \n",
    "    # by -loss (after dividing by the number of images used to generate the loss)\n",
    "    avg_loss /= (n_iter_per_epoch * batch_size)\n",
    "    print(\"log p(x) >= {:0.3f}\".format(-1. * avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
