{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood via gradient ascent\n",
    "In this task we generate data form a univariate Gaussian distribution with unknown mean $\\mu$ and precision $\\tau$. We want to estimate the maximum likelihood parameters using an iterative greedy procedure. At each move we calculate the derivative of the log likelihood of a data batch at the current location in parameter space, then move a distance in that direction, and go again\n",
    "\n",
    "We will try several different combinations here. \n",
    "* Gradients\n",
    "   * Eucledian\n",
    "   * Natural gradients\n",
    "* Batch size\n",
    "   * Gradients based on the full sample\n",
    "   * Gradients based on a \"mini-batch\" of 1 sample.\n",
    "   \n",
    "We would hope to see a little bit about the effect of making the different choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data batch generator**\n",
    "\n",
    "Define a generator by `batch_generator = data_generator(data_set, batch_size)`, and follow that up by `sample = next(batch_generator)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(array, batch_size):\n",
    "    \"\"\"Generate batch with respect to array's first axis.\"\"\"\n",
    "    start = 0  # pointer to where we are in iteration\n",
    "    while True:\n",
    "        stop = start + batch_size\n",
    "        diff = stop - array.shape[0]\n",
    "        if diff <= 0:\n",
    "            batch = array[start:stop]\n",
    "            start += batch_size\n",
    "        else:\n",
    "            batch = np.concatenate((array[start:], array[:diff]))\n",
    "            start = diff\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to adjust the learning-rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_learning_rate(_step, _start_lr=1e-3, _divisor=100, _exponent=1.5):\n",
    "    lr_scale = (1. + _step / _divisor) ** (-_exponent)\n",
    "    return lr_scale * _start_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot generator**\n",
    "\n",
    "Function to generate a plot. Lots of freedom here -- what data to use to get a likelihood contour, should we show the trace through the parameter landscape, should we add the empirical ML estimates calculated in closed form, should we have the gradients indicated in the plot, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(data_for_likelihhood_contour, full_data=None, mu_list=None, tau_list=None,\n",
    "                  gradient_values=None,\n",
    "                  current_mu=None, current_tau=None, title=None):\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close('all')\n",
    "\n",
    "    # Generate a contour-plot using the data se nt in. This will either be a batch or the full dataset\n",
    "    mu_mesh, tau_mesh = np.meshgrid(np.linspace(-2, 2, 100),\n",
    "                                    np.linspace(1E-6, 2, 100))\n",
    "    log_likelihood = np.zeros_like(mu_mesh)\n",
    "    for i in range(len(data_for_likelihhood_contour)):\n",
    "        log_likelihood += norm.logpdf(data_for_likelihhood_contour[i], loc=mu_mesh, scale=1. / np.sqrt(tau_mesh))\n",
    "    plt.contour(mu_mesh, tau_mesh, log_likelihood, 100)\n",
    "\n",
    "    # If the function receives values for mu_list and tau_list, then plot the trace\n",
    "    if mu_list is not None and tau_list is not None:\n",
    "        for point in range(step):\n",
    "            plt.plot(mu_list[point:point + 2], tau_list[point:point + 2], 'g--')\n",
    "\n",
    "    # Add gradient vector to plot if we have the info\n",
    "    if gradient_values is not None:\n",
    "        # Normalize for visual reasons\n",
    "        scale = .25 / np.sqrt(np.sum(np.square(gradient_values)))\n",
    "        plt.gca()\n",
    "        plt.plot(\n",
    "            [current_mu, current_mu + scale * gradient_values[0]],\n",
    "            [current_tau, current_tau + scale * gradient_values[1]],\n",
    "            'b-')\n",
    "\n",
    "    # Add point before update\n",
    "    if current_mu is not None and current_tau is not None:\n",
    "        plt.gca()\n",
    "        plt.plot(current_mu, current_tau, 'ro')\n",
    "\n",
    "    # Add max likelihood params\n",
    "    if full_data is not None:\n",
    "        best_mean = np.mean(full_data)\n",
    "        best_prec = 1. / np.cov(full_data)\n",
    "        plt.plot(best_mean, best_prec, 'k*')\n",
    "\n",
    "    # Pimp up the figure\n",
    "    plt.xlabel('Expectation $\\mu$')\n",
    "    plt.ylabel('Precision $\\\\tau$')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate log likelihood**\n",
    "\n",
    "Calculate the log likelihood of a solution (`_mu`, `_tau`). This is quite straight forward, it is simply the log likelihood of a Gaussian with known parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(_data, _mu, _tau):\n",
    "    return np.sum(norm(loc=_mu, scale=1./np.sqrt(_tau)).logpdf(_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate gradients  \n",
    "\n",
    "This function taeks as input the data-batch `_data` and the location (`_mu`, `_tau`) at  which the gradient is to be calculated. The function returns two lists, the natural gradients (wrt mu first, then wrt. tau) and the euclidean gradients (same order mu, tau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(N, _data, _mu, _tau):\n",
    "    partial_mu = _tau * np.sum(_data - _mu)\n",
    "    partial_tau = .5 * len(_data) / _tau - .5 * np.sum(np.square(_data - _mu))\n",
    "\n",
    "    euclidean_gradient = N / _data.shape[0] * np.array([partial_mu, partial_tau])\n",
    "\n",
    "    # Multiply by inverse Fisher matrix. Fisher matrix is\n",
    "    # diagonal(tau, .5 / tau**2) so inverse is diag( 1/tau, 2 * tau**2 )\n",
    "    partial_mu = partial_mu / _tau\n",
    "    partial_tau = partial_tau * 2 * _tau * _tau\n",
    "\n",
    "    natural_gradient = np.array([partial_mu, partial_tau])\n",
    "\n",
    "    return natural_gradient, euclidean_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup. \n",
    "## The important things here are `batch_size` and `do_natural_gradient`.\n",
    "## Play around with both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size. \n",
    "# This should either be 1 (mini-batching) or everything (full_data.shape[0])\n",
    "batch_size = 1    # Alternatively full_data.shape[0]\n",
    "\n",
    "# Set parameters\n",
    "do_natural_gradient = False\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(123)\n",
    "full_data = np.random.randn(100)\n",
    "batch_generator = data_generator(full_data, batch_size)\n",
    "\n",
    "# Set starting position\n",
    "current_mu = -1\n",
    "current_tau = .1\n",
    "\n",
    "# Set the initial learning rate\n",
    "initial_learning_rate = 1E-3\n",
    "\n",
    "# Store the moves made and the log_likelihood of each intermediuate solution in a list. Used for plotting\n",
    "mu_list = [current_mu]\n",
    "tau_list = [current_tau]\n",
    "total_examples_seen = [0]\n",
    "log_likelihoods = [calculate_log_likelihood(_data=full_data, _mu=current_mu, _tau=current_tau)]\n",
    "\n",
    "# Label-text\n",
    "pic_text = 'Natural' if do_natural_gradient is True else 'Euclidian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(101):\n",
    "    learning_rate = calculate_learning_rate(_step=step, _start_lr=initial_learning_rate, _exponent=1.1)\n",
    "    sample = next(batch_generator)\n",
    "    nat_grad, euc_grad = calculate_gradient(N=full_data.shape[0], _data=sample, _mu=current_mu, _tau=current_tau)\n",
    "    grad = nat_grad if do_natural_gradient is True else euc_grad\n",
    "\n",
    "    if (step % 25) == 0:\n",
    "        generate_plot(data_for_likelihhood_contour=sample, full_data=full_data,\n",
    "                      mu_list=mu_list, tau_list=tau_list,\n",
    "                      gradient_values=grad,\n",
    "                      current_mu=current_mu, current_tau=current_tau,\n",
    "                      title='{:s} gradients; batch={:d}; iter {:d}'.format(pic_text, batch_size, step)\n",
    "                     )\n",
    "\n",
    "    current_mu += learning_rate * grad[0]\n",
    "    current_tau += learning_rate * grad[1]\n",
    "\n",
    "    mu_list.append(current_mu)\n",
    "    tau_list.append(current_tau)\n",
    "    \n",
    "    total_examples_seen.append(batch_size + total_examples_seen[-1])\n",
    "    log_likelihoods.append(calculate_log_likelihood(_data=full_data, _mu=current_mu, _tau=current_tau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show off with the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are done. Show the final plot, where the empiricalÃ¸ log likelihood profile of the full dataset\n",
    "# is shown together with the movements, the final location (that is random if we do small-batch), and\n",
    "# the empirical max likelihood parameters.\n",
    "generate_plot(data_for_likelihhood_contour=full_data, full_data=full_data,\n",
    "              mu_list=mu_list, tau_list=tau_list,\n",
    "              gradient_values=None,\n",
    "              current_mu=current_mu, current_tau=current_tau,\n",
    "              title ='{:s} gradients; batch={:d}; Done'.format(pic_text, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_examples_seen, log_likelihoods)\n",
    "plt.grid(True)\n",
    "plt.title(\"Log likelihood as a function of examples seen\")\n",
    "plt.xlabel(\"Number of examples seen\")\n",
    "plt.ylabel(\"Log likelihood\")\n",
    "plt.xlim([0, total_examples_seen[-1] - 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
