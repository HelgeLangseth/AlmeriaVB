{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Gibbs sampling in a simple Linear Gaussian model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specifiction\n",
    "In this exercise, we consider a model with three continuous variables related as illustrated in the Bayesian network model below\n",
    "<img src=\"model.png\" width=150>\n",
    "We shall assume that the model is specified as follows:\n",
    "<ul>\n",
    "<li> $f(x) = \\mathcal{N}(x|\\mu_x,\\sigma_x^2)$\n",
    "<li> $f(y|x) = \\mathcal{N}(y|x, \\sigma^2)$\n",
    "<li> $f(z|x) = \\mathcal{N}(z|x, \\sigma^2)$\n",
    "</ul>\n",
    "As a starting point, we set the parameters as follows (feel free to experiment with other values)\n",
    "<ul>\n",
    "<li> $\\mu_x=0$\n",
    "<li> $\\sigma_x^2=1$\n",
    "<li> $\\sigma^2 = 1$\n",
    "</ul>\n",
    "and assume that we observe $z=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "We use `import` to get access to libraries that are not originally there for us. Download them to your computer using `pip install`, then load them into the program with `import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm, gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model parmeters according to the specification above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean for x\n",
    "mu_x = 0\n",
    "\n",
    "# The std.dev for x, y and z (y and z have the same variance, called `sigma`)\n",
    "sigma_x = 1\n",
    "sigma = 2\n",
    "\n",
    "# The observed value for z\n",
    "obs_z = 2\n",
    "\n",
    "# Then we set the seed for teh sampler, just to controlwhat goes on\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs sampler\n",
    "\n",
    "For implementing the Gibbs sampler for the example above, we need $f(x|y,z)$ and $f(y|x,z)$. The latter is easy, since the model's independence assumptions imply that\n",
    "$$ f(y|x,z) = f(y|x),$$\n",
    "which we get from the model specification above.\n",
    "\n",
    "With a bit of pencil pushing, we can derive the following expression for $f(x|y,z)$:\n",
    "$$\n",
    "f(x|y,z) = \\mathcal{N}\\left(x|\\frac{\\sigma_x^2}{\\sigma_x^2 + \\sigma^2/2}\\cdot \\frac{y+z}{2} + \\frac{\\sigma^2/2}{\\sigma_x^2 + \\sigma^2/2}\\cdot \\mu_x, \\left (\\frac{1}{\\sigma_x^2} + \\frac{2}{\\sigma^2}\\right )^{-1}\\right ).\n",
    "$$\n",
    "That is, the conditional mean is $\\frac{\\sigma_x^2}{\\sigma_x^2 + \\sigma^2/2}\\cdot \\frac{y+z}{2} + \\frac{\\sigma^2/2}{\\sigma_x^2 + \\sigma^2/2}\\cdot \\mu_x$, and the conditional variance is \n",
    "$\\left(\\frac{1}{\\sigma_x^2} + \\frac{2}{\\sigma^2}\\right )^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Gibbs sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of samples to generate\n",
    "num_iter = 200\n",
    "\n",
    "# The burn - in specifies the number of samples to discard before calculating relevant statistics from \n",
    "# the generated samples. This is to ensure that \"silly\" starting position from teh chain does not interfer\n",
    "# with the generated results.\n",
    "burn_in = 50\n",
    "\n",
    "# The data structure holding the x samples is an array defined by the numpy lib. \n",
    "# We generate it by simply allocating zeros\n",
    "# The first entry is initilized by a random sample \n",
    "# from a standard normal distribution. \n",
    "x_samples = np.zeros(num_iter + 1)\n",
    "x_samples[0] = np.random.normal(loc=0, scale=1)\n",
    "\n",
    "# The index of the most recently sampled value for x\n",
    "last_x_idx = 0\n",
    "\n",
    "# The data structure holding the y samples is again a numpy array. \n",
    "# The first entry is initilized by a random sample \n",
    "# from a standard normal distribution\n",
    "y_samples = np.zeros(num_iter + 1)\n",
    "y_samples[0] = np.random.normal(loc=0, scale=1)\n",
    "\n",
    "\n",
    "# The index of the most recently sampled value for y\n",
    "last_y_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample value for x\n",
    "`def sample_x` is the Python def of a function named `def sample_x`. It does not take any inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_x():\n",
    "    # We use the last_x_idx, and define it as global to ensure it is the same as the outer scope \n",
    "    global last_x_idx\n",
    "        \n",
    "    # Posterior mean\n",
    "    mean = ?????\n",
    "    \n",
    "    # Posterior variance\n",
    "    var = ?????\n",
    "    \n",
    "    # Increment counter \n",
    "    last_x_idx = last_x_idx + 1\n",
    "    \n",
    "    # Sample value, and put in the list\n",
    "    x_samples[last_x_idx] = np.random.normal(loc=mean, scale=np.sqrt(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample value for y\n",
    "We define a similar function for sampling $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_y():\n",
    "    global last_y_idx\n",
    "    last_y_idx = last_y_idx + 1\n",
    "    y_samples[last_y_idx] = np.random.normal(\n",
    "        loc=?????, \n",
    "        scale=?????\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the samler\n",
    "Now we are ready, sample $X_{t+1}$ conditioned on the values  $\\{Z=z, Y=y_t\\}$, \n",
    "then $Y_{t+1}$ based on values  $\\{Z=z, X=X_{t+1}\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    sample_x()\n",
    "    sample_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate values\n",
    "Print out a couple of summary statistics. numpy provides us with these functionalities directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mean and marginal variance for X and Z\n",
    "mean_x = np.mean(x_samples[burn_in:])\n",
    "print(\"Mean(x) = {} \".format(mean_x))\n",
    "var_x = np.var(x_samples[burn_in:])\n",
    "print(\"Var(x) = {} \".format(var_x))\n",
    "mean_y = np.mean(y_samples[burn_in:])\n",
    "print(\"Mean(y) = {} \".format(mean_y))\n",
    "var_y = np.var(y_samples[burn_in:])\n",
    "print(\"Var(y) = {} \".format(var_y))\n",
    "\n",
    "# Covariance\n",
    "print(\"Approximate covariance:\")\n",
    "cov = np.cov([x_samples[burn_in:], y_samples[burn_in:]])\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact values\n",
    "We can find the posterior exactly by just some pencil pushing. The values are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_post_mean_x = sigma_x/(sigma_x + sigma) * obs_z + sigma/(sigma_x + sigma) * mu_x\n",
    "print(\"True posterior mean for x: {}\".format(true_post_mean_x))\n",
    "true_post_var_x = 1.0/(1/sigma_x + 1/sigma)\n",
    "print(\"True posterior variance for x: {}\".format(true_post_var_x))\n",
    "true_post_mean_y = sigma_x/(sigma_x + sigma) * obs_z + sigma/(sigma_x + sigma) * mu_x\n",
    "print(\"True posterior mean for x: {}\".format(true_post_mean_y))\n",
    "true_post_var_y = true_post_var_x + sigma\n",
    "print(\"True posterior variance for x: {}\".format(true_post_var_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute error of mean for x: {}\".format(np.abs(true_post_mean_x)-mean_x))\n",
    "print(\"Absolute error of variance for x: {}\".format(np.abs(true_post_var_x)-var_x))\n",
    "print(\"Absolute error of mean for y: {}\".format(np.abs(true_post_mean_y)-mean_y))\n",
    "print(\"Absolute error of variance for y: {}\".format(np.abs(true_post_var_y)-var_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plt(loc, cov):\n",
    "    x = np.linspace(np.min(x_samples), np.max(x_samples), 100)\n",
    "    y = np.linspace(np.min(y_samples), np.max(y_samples), 100)\n",
    "    x_mesh, y_mesh = np.meshgrid(x, y)\n",
    "    pos = np.empty(x_mesh.shape + (2,))\n",
    "    pos[:, :, 0] = x_mesh\n",
    "    pos[:, :, 1] = y_mesh\n",
    "\n",
    "    rv = multivariate_normal(loc, cov)\n",
    "    plt.contour(x, y, rv.pdf(pos), colors='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We only plot every 10th sample and overlay with the estimated density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notive the notation here. We \"slice\" a vector using x[a:b] to get elements starting \n",
    "# from <a> and going up to but not including <b>. Thus, x_samples[burn_in:] would discard the \n",
    "# first <burn_in> elements. The aditional \":10\" says we only want every tenth sample. \n",
    "# This is to avoid clutter here, but is sometimes also used to reduce auto-correlation \n",
    "# in the sampled sequence.\n",
    "\n",
    "plt.plot(x_samples[burn_in::10], y_samples[burn_in::10],'.r')\n",
    "density_plt([mean_x, mean_y], cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, plot tkhe kernel density approximation for $Y|Z=z$ and compare to the exact one\n",
    "We generate the kernel density by relying on `gaussian_kde`from `scipy`, and compare the approximation (in blue)\n",
    "to the exact result (with pdf calculation again picked out of `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gaussian_kde(y_samples[burn_in:])\n",
    "x_axis_values = np.linspace(-5,8,100)\n",
    "plt.plot(x_axis_values, kernel(x_axis_values), 'b-')\n",
    "plt.plot(x_axis_values, norm.pdf(x_axis_values, loc=true_post_mean_y, scale=np.sqrt(true_post_var_y)), 'r--')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
