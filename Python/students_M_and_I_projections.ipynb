{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment and Information projections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import stuff we need. To get this to run one needs to pip install scipy, matplotlib, and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target distribution -- a Mixture of Gaussians\n",
    "This is the \"p-distribution\", the distribution we want to approximate. The function takes input x, the point at which we evaluate the distribution, plus a number of optional variables. The optional variables are not used. The reason why the function has the option for parameters anyway is that it now gets the same interface as \"q_distribution\" below.\n",
    "\n",
    "The distribution defined is a Mixture-Of-Gaussians with two kernels.\n",
    "Change the behaviour of the function simply by changing the parameters called `w` (weight of first Gaussian component), `mean1`, `stddev1`, `mean2` and `stddev2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_distribution_pdf(x, **kwargs):\n",
    "    \"\"\"This is the target distribution. It only uses x, but is equipped with kwargs\n",
    "    to simplify interface -- now p_distribution and q_distribution (defined below) are exchangeable \"\"\"\n",
    "    w = .1\n",
    "    mean1 = 1\n",
    "    stddev1 = .5\n",
    "    mean2 = 10\n",
    "    stddev2 = 3\n",
    "    \n",
    "    return w * norm.pdf(x, loc=mean1, scale=stddev1) + (1-w) * norm.pdf(x, loc=mean2, scale=stddev2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a fancy plot\n",
    "Note that if the parameters of the p_distribution as described above are changed from their initial values (.1, 1, .5, 10, and  3, respectively) the plot here and below may not look good -- the range of the $x$-axis is hardcoded to be [-5, +25]. For simplicity of implementation below, make sure that all the probability mass of your target distribution is within this range. \n",
    "\n",
    "### --> If the plot looks bad you should not change the parameters as drastically as you have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.arange(-5, +25, .1)\n",
    "plt.figure(figsize=(18, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x_values, p_distribution_pdf(x_values), 'b-')\n",
    "plt.xlim([np.min(x_values), np.max(x_values)])\n",
    "plt.ylim([0, plt.ylim()[1]])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define approximate distribution -- a single Gaussian with unknown loc (mean) and scale (standard deviation)\n",
    "The q_distribution is our approximation. We send in `loc` (mean) and `scale` (std.dev). Hence a call to this function will be like `q_distribution(x=1., loc=0.1, scale=2.)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_distribution_pdf(x, **kwargs):\n",
    "    \"\"\"This is the pdf of approximate distribution.\n",
    "    We assume that to be Normal, and send in loc and scale through the kwargs\"\"\"\n",
    "    loc = kwargs.get('loc')\n",
    "    scale = kwargs.get('scale')\n",
    "    return np.maximum(1e-6, norm.pdf(x=x, loc=loc, scale=scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The general vehicle for optimization: Do numerical optimization of a KL divergence wrt location and scale of the q-distribution. \n",
    "KL calculation can be made in a standard (and very un-optimized) way by implementing naive numerical integration. Note the general interface: It takes a function f, a function g and a value indicating how \"exact\" to be during numerical optiization. The internal function `calculate_KL_for_given_parameters` will return the numerical approimation of the integral $\\int_x f(x) \\log( f(x)/g(x) ) \\, dx$ for given parameters $\\mu$ and $\\sigma$. \n",
    "\n",
    "The functions `f` and `g` should point to either `p_distribution_pdf` or `q_distribution_pdf` defined above, so that one uses the $p$-distribution, the other the $q$-distribution. The code in this function does not know if it is `f` or `g` that points to `q_distribution_pdf`, which is where the parameters come into play. Therefore, we send the parameters to evaluation of both `f` and `g` (this works, because we defined the interface of `p_distribution_pdf` and `q_distribution_pdf` identically, even though only `q_distribution_pdf` utilizes the parameters.)\n",
    "\n",
    "Finally, optimize_parameters will kcik off `minimize`, which is imported from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(f, g, eps):\n",
    "    \"\"\"\n",
    "    The work-horse for the optimization. Takes two functions f and g as input.\n",
    "    One of these is the p-function above, the other is the q-function.\n",
    "    This method does not care which is which.\n",
    "    (Implementation trick: Note that while both the p-func and the q-func accept loc and scale as input through\n",
    "    **kwargs, only the q-function actually uses that input. This, however, ensures that we in this function can\n",
    "    consider p and q as \"exchangeable\").\n",
    "    The function  calculates KL(f||g) through (naive) numerical integration, and returns the mu-sigma-pair that\n",
    "    minimizes the KL.\n",
    "    :param f: An executable function that accepts inputs x, loc, shape which returns a vector of the same shape as x\n",
    "    :param g: An executable function that accepts inputs x, loc, shape which returns a vector of the same shape as x\n",
    "    :param eps: Step-size for the numerical integration\n",
    "    :return: Parameters - a vector of [mu, sigma]\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_KL_for_given_parameters(parameters):\n",
    "        \"\"\"\n",
    "        Calculate the KL(f||g) using the given parameters. \n",
    "        Both f and g are already set (in the surrounding scope); \n",
    "        one is p_distribution the other is q_distribuiton. eps is also set in the outer scope. \n",
    "        This is important beause the interface to scipy's minimize requires you send in a function \n",
    "        that *only* takes a list containing the parameters we try to optimize as input.\n",
    "        \n",
    "        Do the calculation using numeric integration: \n",
    "        At several equally-distanced points along the x-axis, (called x_i, =0, 1, ...) we calculate \n",
    "        f(x_i) * log(f(x_i) / g(x_i))\n",
    "        Then sum all these values, and multiply by the length between x_i and x_(i+1). \n",
    "        Here np.sum() is helpful: It takes the elements of a list/vector and sums the contributions.\n",
    "        np.log(y) will calculate the log of y.\n",
    "        \n",
    "        :param params: Parameter vector: [mu, sigma]\n",
    "        :return: KL(f||g) \n",
    "        \"\"\"\n",
    "        mu = parameters[0]\n",
    "        sigma = parameters[1]\n",
    "\n",
    "        # This will generate a list x_val of values along the x-axis that are eps apart. \n",
    "        # Our area of integration is hardcoded as [-25, +25]. \n",
    "        x_val = np.arange(-25, +25, eps)\n",
    "        \n",
    "        # Do the required calculation:\n",
    "        answer = eps * np.sum(   ??????     )\n",
    "        return answer\n",
    "    \n",
    "    \"\"\"\n",
    "    Find (approximate) optimal value using numerical optimization. \n",
    "    The minimize function from scipy will do this for us. \n",
    "    The call to minimize takes the following parameters:\n",
    "        * a function to be minimized\n",
    "        * bounds for the solution, a touple with one element per parameter, with lower and uper bounds.\n",
    "               Here loc can be whatever, but for numerical stability it can be clever to ensure that \n",
    "               scale must be >= 1E-2 or something like that.\n",
    "        * x0: The startingpoint for the optimization. Almost any value should work here, but it is useful\n",
    "               to supply \"something\" to ensure that the startingpoint is legal (i.e., variance > 0)\n",
    "    \"\"\"\n",
    "    optimum = minimize(calculate_KL_for_given_parameters, \n",
    "                       bounds=((None, None), (1E-2, None)), \n",
    "                       x0=[0, 5])\n",
    "    \n",
    "    # We only need the optimum, not all the extra info (like the minimal KL value), \n",
    "    # hence we only return the \"x-slot\", optimum['x']\n",
    "    return optimum['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KL(p||q) wrt q to get to the so-called moment estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moment_projection(eps=1E-2):\n",
    "    \"\"\"\n",
    "    Generate the \"Moment projection\" (M-projection; Koller & Friedman Def 8.4).\n",
    "    This minimizes KL(p||q) wrt. q. Also known as the \"expectation propagation loss\".\n",
    "    :param eps: The step-size for the numerical integration\n",
    "    :return: Parameters - a vector of [mu, sigma]\n",
    "    \"\"\"\n",
    "\n",
    "    result = optimize_parameters(\n",
    "        f=?????,\n",
    "        g=?????,\n",
    "        eps=eps)\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KL(q||p) wrt q to get to the information estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_information_projection(eps=1E-2):\n",
    "    \"\"\"\n",
    "    Generate the \"Information projection\" (I-projection; K&F Def 8.4).\n",
    "    This minimizes KL(q||p) wrt. q. Also known as \"variational inference\".\n",
    "    :param eps: The step-size for the numerical integration\n",
    "    :param starting_point: Starting point for the optimization.\n",
    "    Anything reasonable will work.\n",
    "    :return: Parameters - a vector of [mu, sigma]\n",
    "    \"\"\"\n",
    "    result = optimize_parameters(\n",
    "        f=?????,\n",
    "        g=?????,\n",
    "        eps=eps)\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the work: Find both moment and information estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find moment estimate\n",
    "moment_estimate = generate_moment_projection()\n",
    "print(\"M-projection: mu = {:5.3f}, sigma = {:5.3f}\".format(\n",
    "    moment_estimate[0], moment_estimate[1]))\n",
    "\n",
    "# Find information estimate\n",
    "information_estimate = generate_information_projection()\n",
    "print(\"I-projection: mu = {:5.3f}, sigma = {:5.3f}\".format(\n",
    "    information_estimate[0], information_estimate[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make nice plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.arange(-5, +25, .01)\n",
    "plt.figure(figsize=(18, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x_values, p_distribution_pdf(x_values), 'b-', label='$p(x)$')\n",
    "plt.plot(x_values, q_distribution_pdf(x_values, loc=moment_estimate[0], scale=moment_estimate[1]),\n",
    "         'r--', label='M-proj: $\\\\arg\\\\min_q KL(p||q)$')\n",
    "plt.plot(x_values, q_distribution_pdf(x_values, loc=information_estimate[0], scale=information_estimate[1]),\n",
    "         'g--', label='I-proj: $\\\\arg\\\\min_q KL(q||p)$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.gca().axes.xaxis.set_ticklabels([])\n",
    "plt.xlim([np.min(x_values), np.max(x_values)])\n",
    "plt.ylim([0, plt.ylim()[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
